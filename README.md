# Trading-Aware Agents in Sugarscape

**Author:** Binh Lai  
**Affiliation:** University of Vaasa, Finland  
**Correspondence:** binh.lai@uwasa.fi  

This repository accompanies the paper  
**â€œTrading-Aware Agents in Sugarscape: A Deep Reinforcement Learning Approach to Adaptive Economic Behaviorâ€**  
(submitted to *Computational Economics*, Springer).

---

## ğŸ§­ Overview
This project extends the classical **Sugarscape** model by integrating **Deep Reinforcement Learning (DRL)** to jointly optimize movement and trading decisions.  
Agents learn adaptive strategies via **Proximal Policy Optimization (PPO)**, closing the traditional *â€œmoveâ€“thenâ€“tradeâ€* gap and producing emergent equilibria consistent with economic theory.

### Key Features
- **Environment:** Unity-based Sugarscape world with dual renewable resources (sugar & spice).  
- **Learning Algorithm:** Parameter-sharing PPO with centralized training and decentralized execution.  
- **Behavioral Regimes:**  
  1. *Cobbâ€“Douglas Utility Scheme* â€” welfare-maximizing consumption behavior.  
  2. *Kinked Survival Utility Scheme* â€” lexicographic survival-first behavior.  
- **Metrics:** Carrying capacity, market-price stability, welfare efficiency, and inequality (Gini & Pareto indices).  
- **Policy Experiments:** Optional transaction-tax analysis (Appendix A of the paper).

---

## ğŸ“ Repository Structure
```
Sugarscape-DRL/
â”‚
â”œâ”€â”€ README.md                 â† this file
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ training_env/              â† Unity training environment
â”‚   â”œâ”€â”€ training_env_64bit
â”‚   â””â”€â”€ training_env_silicon
â”‚
â”œâ”€â”€ trained_models/
â”‚   â”œâ”€â”€ cobb_douglas_reward.onnx      â† trained model for cobbcobb_douglas_reward
â”‚   â””â”€â”€ kinked_survival_reward.onnx   â† trained model for kinked_survival_reward
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ cobb_douglas_reward.yaml  â† configuration for cobbcobb_douglas_reward
â”‚   â””â”€â”€ kinked_survival_reward.yaml â† configuration for kinked_survival_reward
â”‚
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/Nanyka/trading_aware_agents_sugarscape.git
cd trading_aware_agents_sugarscape
```

### 2ï¸âƒ£ Install dependencies
```bash
pip install -r training/requirements.txt
```
*Required packages:* `mlagents`, `torch`, `numpy`, `matplotlib`, `pandas`, `seaborn`, `notebook`.

### 3ï¸âƒ£ Create & activate a virtual environment

This guide provides a **reproducible** path to train a Unity MLâ€‘Agents project using **Conda** environments. It is suitable for reviewers who need a clean setup on macOS (Apple Silicon or Intel) or Linux.

```bash
conda create -n py3923 python=3.9.23 -y
conda activate py3923
pip install -r requirements.txt 
cd <project_local_directory>
```

---

## ğŸš€ Usage

### ğŸ§  Train agents

**Cobb-Douglas Utility Reward**:
```bash
mlagents-learn ./config/cobb_douglas_reward.yaml  --env=./training_env/training_env_silicon.app --run-id=<RUN_ID> --no-graphic
```

**Kinked Survival Reward**:
```bash
mlagents-learn ./config/kinked_survival_reward.yaml  --env=./training_env/training_env_silicon.app --run-id=<RUN_ID> --no-graphic
```

Key parameters (also adjustable in `config.yaml`):
- Population = 500 agents  
- Vision = 10  
- Max steps = 5 Ã— 10â¶  
- Reward scheme = {CobbDouglasUtility | KinkedSurvival}

Use a descriptive value for <RUN_ID> to distinguish and save checkpoints/models for each experiment (e.g., cd_utility_v1, kinked_survival_ablation).
Optional flags you may add: --results-dir ./runs (custom output path), --force (overwrite an existing run).

### ğŸ“Š Run analysis
After training, open the notebook:
```bash
jupyter notebook analysis/analysis_notebook.ipynb
```
The notebook reproduces:
- Carrying-capacity curves  
- Price-stability plots  
- Welfare and inequality metrics  
- Pareto-tail dynamics and spatial patterns  

---

## ğŸ“ˆ Main Results (summary)
| Metric | DRL vs. Rule-based | Description |
|---------|--------------------|--------------|
| **Carrying capacity** | â†‘ 28â€“32 % | DRL agents sustain larger populations. |
| **Price volatility** | â†“ ~50 % | Faster convergence to equilibrium. |
| **Aggregate welfare** | â†‘ 7 % | More efficient resource utilization. |
| **Inequality (Gini)** | â†“ 0.05â€“0.08 | Fairer long-run wealth distribution. |

---

## ğŸ§© Reproducibility Notes
- Deterministic and stochastic resource landscapes are both supported.  
- Each reported result averaged over 50 replications with fixed random seeds.  
- Training reproducible via `train.py` using the provided config.  
- Hardware used: NVIDIA RTX 3090 (24 GB), Intel i9-13900K, 64 GB RAM.  

---

## ğŸ§  Citation
```bibtex
@article{Lai2025SugarscapeDRL,
  author  = {Binh Lai},
  title   = {Trading-Aware Agents in Sugarscape: A Deep Reinforcement Learning Approach to Adaptive Economic Behavior},
  journal = {Computational Economics},
  year    = {2025},
  note    = {Submitted manuscript},
}
```

---

## ğŸ“œ License
This project is released under the **MIT License** â€” see [LICENSE](LICENSE).

---

## ğŸ“‚ Data Availability Statement
The Unity simulation environment, training scripts, and analysis notebooks used in the paper are publicly available at  
ğŸ‘‰ **https://github.com/BinhLai/trading_aware_agents_sugarscape**  
(commit `v1.0_submission`).

---

## ğŸ¤ Acknowledgements
This work was supported by the **University of Vaasa** and the **DigiConsumers Research Network**.  
The author thanks **Prof. Panu Kalmi** and the *Computational Economics* editorial team for constructive feedback on reproducibility and open-science practices.
